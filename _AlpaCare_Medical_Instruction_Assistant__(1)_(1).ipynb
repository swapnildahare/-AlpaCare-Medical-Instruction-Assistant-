{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnildahare/-AlpaCare-Medical-Instruction-Assistant-/blob/main/_AlpaCare_Medical_Instruction_Assistant__(1)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CiO8e5pmveb",
        "outputId": "a82dd03c-0db1-41cc-8196-d77fe73b6705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8KJDvZEnO_T",
        "outputId": "ac408e8e-34c4-4ea9-86eb-f8716d98f917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q transformers accelerate datasets bitsandbytes safetensors evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te6kh-gunWVz"
      },
      "outputs": [],
      "source": [
        "pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsbdHxErneRT"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StTpBF69nsQJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gt85BZ6n8Fv",
        "outputId": "8ce279bb-2ea2-4b00-e471-c8f55c0dc10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers 4.56.2\n",
            "Torch 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "print('Transformers', transformers.__version__)\n",
        "print('Torch', torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "actZNSOpoDa3"
      },
      "outputs": [],
      "source": [
        "ARTIFACT_DIR = Path('/content/adapters')\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPLpXiKbovl_",
        "outputId": "dd753858-bb75-4167-cad4-89f4ac645596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "BASE_MODEL = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
        "\n",
        "MAX_LENGTH = 1024\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUM = 8\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "OUTPUT_DIR = '/content/outputs'\n",
        "ADAPTER_NAME = 'alpacare-lora'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "23ea60d49d0e4e09916b4fe4e9a4aa28",
            "6a84e8eaebc54885b5079b009d50a193",
            "a2e9c61314074130afcf3145c931d5b8",
            "ea85c1473170469985fd375f8f12e75c",
            "08f80b06e70f4bb58af2f29c76139bcb",
            "2338f483017b46fab691ae38f97ce068",
            "035e0786888a4f18964dc971028623d9",
            "b508e016368748749d30e7675c3addb8",
            "781d2a1a835d480bbfa4f05efe9f9341",
            "a7fcf60f7c714d5b8dd9c7bf7e0f4a1a",
            "f669c0917de74244bc7655317aa91b35"
          ]
        },
        "id": "yTkAETHlo5ow",
        "outputId": "bbf5c369-e977-482c-d718-89eccfedb8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset lavita/AlpaCare-MedInstruct-52k...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ea60d49d0e4e09916b4fe4e9a4aa28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'response'],\n",
            "        num_rows: 52002\n",
            "    })\n",
            "})\n",
            "Example record:\n",
            "{'prompt': '### Instruction:\\\\nExplain why a mass in the lung could cause shortness of breath.\\\\n\\\\n### Input:\\\\n<noinput>\\\\n\\\\n### Response:\\\\n', 'response': 'A mass in the lung could cause shortness of breath due to several reasons. First, the mass can physically obstruct the air passages, causing difficulty in airflow and leading to breathing difficulties. Second, if the mass is cancerous or infected, it can cause inflammation and damage to lung tissue, reducing its functional capacity and compromising normal breathing. Additionally, a lung mass can compress adjacent structures such as blood vessels, bronchi, or the diaphragm, further impeding normal respiratory function. Overall, any interference with the normal flow of air in the lungs caused by a mass can result in inadequate oxygen exchange and subsequent shortness of breath.\\n\\nThe answer is: A mass in the lung can obstruct air passages, cause inflammation, damage lung tissue, and compress adjacent structures, leading to shortness of breath.'}\n"
          ]
        }
      ],
      "source": [
        "print('Loading dataset lavita/AlpaCare-MedInstruct-52k...')\n",
        "dataset = load_dataset('lavita/AlpaCare-MedInstruct-52k')\n",
        "\n",
        "def make_prompt(example):\n",
        "    instruction = example.get('instruction') or example.get('prompt') or ''\n",
        "    inp = example.get('input') or ''\n",
        "    output = example.get('output') or example.get('response') or example.get('answer') or ''\n",
        "    if inp.strip():\n",
        "        return {\"prompt\": f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Input:\\\\n{inp}\\\\n\\\\n### Response:\\\\n\", \"response\": output}\n",
        "    return {\"prompt\": f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n\", \"response\": output}\n",
        "\n",
        "dataset['train'] = dataset['train'].map(make_prompt, remove_columns=dataset['train'].column_names)\n",
        "\n",
        "\n",
        "print(dataset)\n",
        "print('Example record:')\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "77ae86bdf36442ac88daefbd285bbde2",
            "339eb19b07974369a20d7c7baac396c6",
            "4ab6113e4b3648f289c1e79db9113cf6",
            "b2c840841cd545849398394c3269d73b",
            "1792a16a4b754d1ca120f728e23e73d0",
            "d0be055378ff43cc8e78995908ebb725",
            "18bd297e51cf48ee8680fc33b9a3427b",
            "bff80b0d41c64ea89d539c70f478e145",
            "37f2dbd435fb4c89832c41c1fafa648d",
            "24ff1ce367294c3da19153e6a30d4154",
            "9e0b194ecf204e68a3fddf1a2a2cf69f"
          ]
        },
        "id": "tJKm3Illo9N4",
        "outputId": "27e37056-cdff-44b1-b056-a91b1cb19873"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77ae86bdf36442ac88daefbd285bbde2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import Dataset, concatenate_datasets\n",
        "# removed the redundant dataset['train'] = dataset['train'].map(...) line\n",
        "dataset = {split: dataset[split].map(make_prompt, remove_columns=dataset[split].column_names) for split in dataset if split in ['train', 'test']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIv8cv5OsIZR"
      },
      "outputs": [],
      "source": [
        "#  small curated refusal dataset to teach safe refusals\n",
        "refusal_examples = [\n",
        "{\n",
        "'prompt': \"### Instruction:\\nI have chest pain and shortness of breath, what is wrong with me?\\n\\n\\n### Response:\\n\",\n",
        "'response': (\"I’m sorry — I can’t provide medical diagnoses. I can offer general instructional information about first aid and when to contact medical services. \"\n",
        "\"If you or someone else may be experiencing a medical emergency (difficulty breathing, severe chest pain, fainting, sudden confusion), please call your local emergency number immediately.\"),\n",
        "},\n",
        "{\n",
        "'prompt': \"### Instruction:\\nCan you prescribe a medication for my fever?\\n\\n\\n### Response:\\n\",\n",
        "'response': (\"I can’t prescribe medications. Medication prescriptions must come from a licensed healthcare professional after an assessment. \"\n",
        "\"If you are unwell, please consult a doctor or pharmacist.\")\n",
        "}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwpCATxzsxGg",
        "outputId": "9e99e527-b912-4702-df23-d92109749e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training size after adding refusals: 52004\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, concatenate_datasets\n",
        "ref_ds = Dataset.from_list(refusal_examples)\n",
        "dataset['train'] = concatenate_datasets([dataset['train'], ref_ds])\n",
        "print('Training size after adding refusals:', len(dataset['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "2489281c8e7d4588939414464b5847cd",
            "73c7292113fa46afab21df2fce274a5d",
            "063d52c9a746466994409bdc29609965",
            "228fc89445f04d22a846f2edd35036b0",
            "96b63e601f9e47c7918a8b93dc518b82",
            "cd60f8af6dcb4be88cb8a2dc5b08efc7",
            "6f28b810f8a34325bc9c9f8f083ab72d",
            "61dc8225f52f4de395a7deadd5868a6b",
            "d87923cc7f6640949db388f6d6bfa6bb",
            "fdc5cbe5a37d440a859097a518c53828",
            "9b86c2bd782449049c86df1c33193fba",
            "60097d9d16cd4deeafd46d9b5ff72f78",
            "85e19885455641149e00930fd8aef249",
            "bfe6345303c9464a8523058922f2906f",
            "7aa7b3d3d5694cd6ac2ef32839d3b18f",
            "b68d66ce5f8a457c8baaff10cda79a47",
            "73d80da5fe0b4a1b970537669afa1a37",
            "be59d7d8965547ec9d17b4ac1137e870",
            "03fed6a5591749408a1c1907f94920f2",
            "94333d15f8834a008717c56c8e5cb16d",
            "555af591f8734a899d7200477b0fd86b",
            "2b749d2c25ec42d1908285113f13b5ba",
            "b8476d5285de4ce88e80bcb771a58d1b",
            "4230b0546a254281907b761b5cfec185",
            "5064d8d9528e43a0a5e52ccde8de9013",
            "f8140e258e554bd0ac8e4390e4b41316",
            "8a99f1428131414e8813ede8ac872926",
            "2cb4b469d1b44290a35e4882afcec5e1",
            "2447f0e5e0754a20b3dd248d5ba0846e",
            "b0118f1b614b47899145924487309119",
            "e9afe49c0bee4963816c658aceaf7032",
            "bdf8c262b71a4054a776cbdfe227ca76",
            "412bbf7b1a534d4b8858663cc2968918"
          ]
        },
        "id": "EsI4RW1ctYQh",
        "outputId": "9a3d851d-8341-487a-d896-caf862dd5d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for togethercomputer/RedPajama-INCITE-Chat-3B-v1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2489281c8e7d4588939414464b5847cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60097d9d16cd4deeafd46d9b5ff72f78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8476d5285de4ce88e80bcb771a58d1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('Loading tokenizer for', BASE_MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "   tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6383c78",
        "outputId": "11ba2b6b-b0b1-4f86-e7c0-758ca818aaed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HE_token successfully loaded and set as environment variable.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "hf_token = userdata.get('HE_token')\n",
        "\n",
        "\n",
        "if hf_token is None:\n",
        "    print(\"HE_TOKEN not found in Colab secrets. Please add your Hugging Face token as a secret named 'HE_token'.\")\n",
        "else:\n",
        "\n",
        "    os.environ['HE_token'] = hf_token\n",
        "    print(\"HE_token successfully loaded and set as environment variable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utpvvn-P0iPF"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 512\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "   input_texts = [p + r for p, r in zip(examples['prompt'], examples['response'])]\n",
        "   tokenized_full = tokenizer(input_texts, truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
        "\n",
        "   prompt_tokenized = tokenizer(examples['prompt'], truncation=True, max_length=MAX_LENGTH)\n",
        "   labels = []\n",
        "   for i in range(len(input_texts)):\n",
        "       ids = tokenized_full['input_ids'][i].copy()\n",
        "       prompt_len = len(prompt_tokenized['input_ids'][i])\n",
        "\n",
        "       ids[:prompt_len] = [-100] * prompt_len\n",
        "       labels.append(ids)\n",
        "\n",
        "   tokenized_full['labels'] = labels\n",
        "   return tokenized_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "4a5121e304b34b1286e82fa9bacf7d8b",
            "2581671ed68e406b82b4ae074e716aab",
            "15620b23052741a4baf62965c6b5d4c6",
            "03357aba86ff4295a4c03b99ce596ca1",
            "1b419a0d57a3470ca357964d527381fb",
            "029cf7938371419ca4f95fa20392f94e",
            "723de1db81954e2ca8b6840164177c0b",
            "dbdb34ccf3b9489dba96271853a2cd82",
            "64b85c778d29420886b99261b1560e1d",
            "c3a6ce80db4b48138aaee0501f9dcd83",
            "0866841252de4c1990b5eee02e07db80"
          ]
        },
        "id": "sEwVLUtJ3ZGH",
        "outputId": "865a0c84-1ecb-4b3e-c43b-28c63d8f99da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a5121e304b34b1286e82fa9bacf7d8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/52004 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sample:\n",
            "{'input_ids': [4118, 41959, 8048, 79, 4118, 41959, 8048, 79, 1672, 19104, 2139, 247, 2280, 275, 253, 6400, 812, 2847, 2159, 1255, 273, 6345, 4880, 79, 61, 79, 4118, 19832, 8048, 79, 29, 2369, 5423, 13544, 79, 61, 79, 4118, 19371, 8048, 79, 61, 79, 61, 79, 4118, 19371, 8048, 79, 34, 2280, 275, 253, 6400, 812, 2847, 2159, 1255, 273, 6345, 1955, 281, 2067, 4606, 15, 3973, 13, 253, 2280, 476, 13318, 23040, 253, 2329, 24392, 13, 8479, 10183, 275, 2329, 5449, 285, 4283, 281, 14578, 12748, 15, 6347, 13, 604, 253, 2280, 310, 2923, 528, 390, 9813, 13, 352, 476, 2847, 10696, 285, 4723, 281, 6400, 4408, 13, 8493, 697, 5164, 5350, 285, 48637, 2622, 14578, 15, 9157, 13, 247, 6400, 2280, 476, 19477, 9701, 5289, 824, 347, 2614, 12671, 13, 13605, 4635, 13, 390, 253, 34622, 13, 2007, 1607, 7022, 2622, 11900, 1159, 15, 15699, 13, 667, 11689, 342, 253, 2622, 2685, 273, 2329, 275, 253, 18926, 4269, 407, 247, 2280, 476, 906, 275, 18766, 7768, 6431, 285, 6774, 2159, 1255, 273, 6345, 15, 187, 187, 510, 3662, 310, 27, 329, 2280, 275, 253, 6400, 476, 23040, 2329, 24392, 13, 2847, 10696, 13, 4723, 6400, 4408, 13, 285, 19477, 9701, 5289, 13, 4283, 281, 2159, 1255, 273, 6345, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34, 2280, 275, 253, 6400, 812, 2847, 2159, 1255, 273, 6345, 1955, 281, 2067, 4606, 15, 3973, 13, 253, 2280, 476, 13318, 23040, 253, 2329, 24392, 13, 8479, 10183, 275, 2329, 5449, 285, 4283, 281, 14578, 12748, 15, 6347, 13, 604, 253, 2280, 310, 2923, 528, 390, 9813, 13, 352, 476, 2847, 10696, 285, 4723, 281, 6400, 4408, 13, 8493, 697, 5164, 5350, 285, 48637, 2622, 14578, 15, 9157, 13, 247, 6400, 2280, 476, 19477, 9701, 5289, 824, 347, 2614, 12671, 13, 13605, 4635, 13, 390, 253, 34622, 13, 2007, 1607, 7022, 2622, 11900, 1159, 15, 15699, 13, 667, 11689, 342, 253, 2622, 2685, 273, 2329, 275, 253, 18926, 4269, 407, 247, 2280, 476, 906, 275, 18766, 7768, 6431, 285, 6774, 2159, 1255, 273, 6345, 15, 187, 187, 510, 3662, 310, 27, 329, 2280, 275, 253, 6400, 476, 23040, 2329, 24392, 13, 2847, 10696, 13, 4723, 6400, 4408, 13, 285, 19477, 9701, 5289, 13, 4283, 281, 2159, 1255, 273, 6345, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "print('Tokenizing dataset...')\n",
        "tokenized = {split: dataset[split].map(tokenize_fn, batched=True, remove_columns=dataset[split].column_names) for split in dataset}\n",
        "print('Tokenized sample:')\n",
        "print({k: tokenized['train'][0][k] for k in ['input_ids','labels']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "b84f86c92dad43089c91847523c8682b",
            "ff41a17e84ac4682aa97c529f27d793e",
            "ecd41f80517e40d59f5a1c3edccd8d0f",
            "a4eecbd076a4430cae0a8164d9ebddfd",
            "023ab8c2a5d647c480524a0ead5d905a",
            "9ae04f24dac549189063f409d7655ba6",
            "ecf5cabf360649048ad0fb17c4b7972f",
            "79a33a6f5a6f4720b80c93fe502bdaae",
            "e913f1a031194735941ae1e2f88d37f8",
            "5ba31f84193d4d6fa60e42cb2232168d",
            "33cb574d8a3e4d00b41af5f81137079b",
            "1815697d258b4f26989539f775e729de",
            "b70f5a6f77384436acbd3ac070ff29aa",
            "dd071dfcdf3a4841835a4d53f8af869b",
            "d4b79d3a9a30458ab965e268bac9a567",
            "f563ce600c084fa0bbc9509e9a40544f",
            "57f6aa1079bd4025a140a5b61ebcdf55",
            "20b79bd201bd4e18b928fbe3f77e80d1",
            "823e3e61a83746f5afad2b04a816e024",
            "9b3e0a0c9d294451bf3c4c8c49a2970c",
            "eda29f6c667a439583896e8fad32c971",
            "7a87f18caff24e158143372782b48721",
            "1fd7b9179ba6436a8fb4d1b1d0368397",
            "ded773e5a7394287a4b8b6174f2f8fbd",
            "d1b5bdca7912473a95cf6210d11ffd84",
            "285b63240186418e9b58152da2bbc316",
            "2518fb046fd74d94b62875f130902d8d",
            "a4991fa7a2754614b8f72dc7eb7b158f",
            "7f367d18dba648989163cdf3de176d96",
            "8c666a0d35c44588a245ee76e0fd0421",
            "a8ab950bd92843e298480fff766565a8",
            "8ce3c85f1a5a480b95da60afc36ab68d",
            "49c547ab6848499cbfd0a9ad53e682a1",
            "e675f1d6bd3f4b90bc6b050b60c72193",
            "d9785bbb923a44c5a6bb16e4893d820b",
            "57e6c49b00a64950b2df7d837729bf3e",
            "2bd8488ce8cc434996573d5b29ce539b",
            "489c358ffba746218ea1f49fdfaa0528",
            "58241f2643594cf885ec4c630d3e97d7",
            "bbda271b21754f26b124ab8304171715",
            "2350dce32e28477585f61d6ea1c6b38c",
            "33192048296c424f8f03d62101731fbb",
            "45b708f140f24432a0470853d4fd9f3a",
            "067351c6634a4f6a862e66ff71c30f05"
          ]
        },
        "id": "vJ17gc-wtemA",
        "outputId": "aa8b7f91-e0c7-49d1-8925-d38d32d2b3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model (8-bit) - this may take a while...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b84f86c92dad43089c91847523c8682b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1815697d258b4f26989539f775e729de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/5.69G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fd7b9179ba6436a8fb4d1b1d0368397",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.69G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e675f1d6bd3f4b90bc6b050b60c72193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('Loading base model (8-bit) - this may take a while...')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "BASE_MODEL,\n",
        "load_in_8bit=True,\n",
        "device_map='auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHG1bUijvqBW"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "TARGET_MODULES = [\"query_key_value\"]\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ5bcivEv1Ll",
        "outputId": "6728fab3-bfc0-4bd7-8f84-0516484ce60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,621,440 || all params: 2,778,485,760 || trainable%: 0.0943\n"
          ]
        }
      ],
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11afdab7",
        "outputId": "4eef43eb-1a8b-4a98-aae2-595c106aa57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPTNeoXForCausalLM(\n",
            "      (gpt_neox): GPTNeoXModel(\n",
            "        (embed_in): Embedding(50432, 2560)\n",
            "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x GPTNeoXLayer(\n",
            "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (attention): GPTNeoXAttention(\n",
            "              (query_key_value): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "            )\n",
            "            (mlp): GPTNeoXMLP(\n",
            "              (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
            "              (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
            "              (act): GELUActivation()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
            "      )\n",
            "      (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EajZp6lYv5Ov"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "output_dir=OUTPUT_DIR,\n",
        "per_device_train_batch_size=BATCH_SIZE,\n",
        "gradient_accumulation_steps=GRAD_ACCUM,\n",
        "num_train_epochs=NUM_EPOCHS,\n",
        "learning_rate=LEARNING_RATE,\n",
        "fp16=True,\n",
        "logging_steps=50,\n",
        "save_total_limit=2,\n",
        "remove_unused_columns=False,\n",
        "report_to='none',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8LdCMe4v7Ht",
        "outputId": "8fba8bfa-6e9e-4965-f347-62f8693650ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-357569606.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "model=model,\n",
        "args=training_args,\n",
        "train_dataset=tokenized['train'],\n",
        "tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3e5f7fb7b4d244b988401be856576ef0",
            "d82db39e85d9445da0c65e6204f1576a",
            "0149f8c98dd442058989d9a0a6e13add",
            "bd510e385d9c4f81808131989e1d2519",
            "4966d0dacb6f48fcbf755d9c6e406d44",
            "bbb4463250e0454fbe0c14204a5d930c",
            "b5d58419ffc04f8f858c7424623f7229",
            "dcedb543850c4e1883eede2b3783a208",
            "8db7492d321b4be98573c8fa2e992707",
            "93b0e514fa31408c9fad81b2f8c5aab6",
            "75d80bc1971d4fc59366625e010eb858"
          ]
        },
        "id": "gKElpsRfy4jV",
        "outputId": "451d84a5-8bde-4c65-cbfa-27abe0e89fba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e5f7fb7b4d244b988401be856576ef0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "subset = dataset[\"train\"].shuffle(seed=42).select(range(500))  # only 500 samples\n",
        "tokenized_small = subset.map(tokenize_fn, batched=True, remove_columns=subset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfvsgTZh2MHW"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,             # 1 epoch\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "a1rnWJGh2Uae",
        "outputId": "cb0e6ca1-e2c8-4d01-c69a-3fe9cb6d8d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1844027803.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 09:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.542700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.573400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.511600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=125, training_loss=0.6991746253967285, metrics={'train_runtime': 582.378, 'train_samples_per_second': 0.859, 'train_steps_per_second': 0.215, 'total_flos': 4069447434240000.0, 'train_loss': 0.6991746253967285, 'epoch': 1.0})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_small,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_7ArQko5Vgi",
        "outputId": "c3fcdf37-efc6-48dd-96ba-b24994d8d1f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter saved to /content/adapters/alpacare-lora\n"
          ]
        }
      ],
      "source": [
        "adapter_path = ARTIFACT_DIR / ADAPTER_NAME\n",
        "adapter_path.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_path)\n",
        "print('Adapter saved to', adapter_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3OW-E4_5WI2",
        "outputId": "21e828da-444e-44b8-db18-caf9bc5a9b03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/adapters/alpacare-lora/tokenizer_config.json',\n",
              " '/content/adapters/alpacare-lora/special_tokens_map.json',\n",
              " '/content/adapters/alpacare-lora/tokenizer.json')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(adapter_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoEx2usD5Yit",
        "outputId": "85d152d4-0bac-4c42-969e-0061637ab5fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model for inference...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPTNeoXForCausalLM(\n",
              "      (gpt_neox): GPTNeoXModel(\n",
              "        (embed_in): Embedding(50432, 2560)\n",
              "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x GPTNeoXLayer(\n",
              "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (attention): GPTNeoXAttention(\n",
              "              (query_key_value): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "            )\n",
              "            (mlp): GPTNeoXMLP(\n",
              "              (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "              (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "      )\n",
              "      (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "\n",
        "print('Loading base model for inference...')\n",
        "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map='auto')\n",
        "model_inf = PeftModel.from_pretrained(base, adapter_path)\n",
        "model_inf.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4RVPxzS5lx8"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "\n",
        "def generate(prompt_text, max_new_tokens=256, do_sample=False):\n",
        "   inputs = tokenizer(prompt_text, return_tensors='pt').to(model_inf.device)\n",
        "   gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=do_sample)\n",
        "   with torch.no_grad():\n",
        "     out = model_inf.generate(**inputs, generation_config=gen_cfg)\n",
        "   text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "   return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCtqz5CI54ps"
      },
      "outputs": [],
      "source": [
        "DIAGNOSTIC_KEYWORDS = ['diagnose', 'diagnosis', \"what's wrong\", 'what is wrong', 'do i have', 'am i sick']\n",
        "PRESCRIPTION_KEYWORDS = ['prescribe', 'prescription', 'dosage', 'mg', 'take mg', 'take this']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfeHDxP5_Wk"
      },
      "outputs": [],
      "source": [
        "def is_diagnostic_request(text):\n",
        "   t = text.lower()\n",
        "   return any(k in t for k in DIAGNOSTIC_KEYWORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XZyPZEm6AZS"
      },
      "outputs": [],
      "source": [
        "def is_prescription_request(text):\n",
        "   t = text.lower()\n",
        "   return any(k in t for k in PRESCRIPTION_KEYWORDS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJonI5g6jTc"
      },
      "outputs": [],
      "source": [
        "def safe_generate(instruction, input_text=''):\n",
        "    combined = instruction + ' ' + (input_text or '')\n",
        "    if is_diagnostic_request(combined) or is_prescription_request(combined):\n",
        "        return (\"I’m not able to provide diagnoses or prescribe medication. I can provide general instructional guidance (for example: how to clean a wound or use a medical device). \"\n",
        "                \"If this is an emergency, please contact local emergency services.\")\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n\\n\"\n",
        "    if input_text:\n",
        "        prompt += f\"### Input:\\n{input_text}\\n\\n\\n\"\n",
        "    prompt += \"### Response:\\n\"\n",
        "    out = generate(prompt)\n",
        "    # strip prompt prefix\n",
        "    if out.startswith(prompt):\n",
        "        return out[len(prompt):].strip()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RZUm9pe7cB7"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "'I have chest pain and shortness of breath, what is wrong with me?',\n",
        "'Can you prescribe me 500 mg of paracetamol for fever?',\n",
        "'How to perform basic wound cleaning and dressing?',\n",
        "'I have a rash and fever; do I need antibiotics?'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI_cko0Y7fX5",
        "outputId": "3bb7d1f7-86ca-4f83-87c2-a40a5cf540e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT: I have chest pain and shortness of breath, what is wrong with me?\n",
            "SAFE RESPONSE: I’m not able to provide diagnoses or prescribe medication. I can provide general instructional guidance (for example: how to clean a wound or use a medical device). If this is an emergency, please contact local emergency services.\n",
            "-------------------------\n",
            "PROMPT: Can you prescribe me 500 mg of paracetamol for fever?\n",
            "SAFE RESPONSE: I’m not able to provide diagnoses or prescribe medication. I can provide general instructional guidance (for example: how to clean a wound or use a medical device). If this is an emergency, please contact local emergency services.\n",
            "-------------------------\n",
            "PROMPT: How to perform basic wound cleaning and dressing?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAFE RESPONSE: 1. Open the wound and inspect it for any signs of infection or bleeding.\n",
            "2. Clean the wound thoroughly with soap and water.\n",
            "3. Gently pat dry with a clean cloth.\n",
            "4. Apply a sterile dressing to cover the wound.\n",
            "5. Cover the dressing with a bandage to prevent further bleeding.\n",
            "6. Monitor the wound regularly for signs of infection or bleeding.\n",
            "7. If necessary, consult a healthcare professional for further advice.\n",
            "\n",
            "Remember to follow these steps carefully and take appropriate precautions to prevent further complications.\n",
            "-------------------------\n",
            "PROMPT: I have a rash and fever; do I need antibiotics?\n",
            "SAFE RESPONSE: If you have a fever and a rash, it is important to see a healthcare provider.\n",
            "\n",
            "Antibiotics are not usually recommended for the treatment of a fever and a rash.\n",
            "\n",
            "The decision to prescribe antibiotics should be based on a thorough evaluation of your symptoms, medical history, and physical examination.\n",
            "\n",
            "It is important to discuss your symptoms with your healthcare provider to determine if they are caused by a bacterial infection or another condition.\n",
            "\n",
            "If your healthcare provider determines that you have a bacterial infection, they may prescribe antibiotics based on the following criteria:\n",
            "\n",
            "1. Presence of a fever:\n",
            "If your fever is higher than 101°F (38.3°C), antibiotics may be prescribed.\n",
            "\n",
            "2. Presence of a rash:\n",
            "If you have a rash, antibiotics may be prescribed if it is accompanied by a fever.\n",
            "\n",
            "3. Presence of other symptoms:\n",
            "Other symptoms such as headache, sore throat, or diarrhea may also indicate a bacterial infection.\n",
            "\n",
            "It is important to note that antibiotics are not effective for viral infections such as the common cold or flu.\n",
            "\n",
            "It is also important to discuss the potential side effects of antibiotics with your healthcare provider.\n",
            "\n",
            "If you have any questions or concerns about your\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "for p in prompts:\n",
        "   print('PROMPT:', p)\n",
        "   print('SAFE RESPONSE:', safe_generate(p))\n",
        "   print('-------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V2Prqp38KGw"
      },
      "outputs": [],
      "source": [
        "!pip install gradio -q\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgUKLB8b8YIe"
      },
      "outputs": [],
      "source": [
        "def chat_with_alpacare(instruction):\n",
        "    return safe_generate(instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "tcLdsrNa8iuc",
        "outputId": "7b7f60df-b292-414c-9f03-1c8b668026bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6f0e4f4fcfd14cd9db.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6f0e4f4fcfd14cd9db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gr.Interface(\n",
        "    fn=chat_with_alpacare,\n",
        "    inputs=gr.Textbox(lines=4, placeholder=\"Enter your medical instruction query...\"),\n",
        "    outputs=gr.Textbox(lines=10),\n",
        "    title=\"🩺 AlpaCare Medical Instruction Assistant\",\n",
        "    description=\"A safe, non-diagnostic assistant fine-tuned with LoRA on AlpaCare-MedInstruct dataset.\"\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM35wZDj-oXP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}